% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
]{article}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.50,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.50,0.00}{\textbf{#1}}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Dimensionality Reduction with Principal Component Analysis (PCA)},
  pdfauthor={GCCP},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Dimensionality Reduction with Principal Component Analysis (PCA)}
\author{GCCP}
\date{February 8, 2026}

\begin{document}
\maketitle

\section{Lecture Notes: Dimensionality Reduction with Principal
Component Analysis
(PCA)}\label{lecture-notes-dimensionality-reduction-with-principal-component-analysis-pca}

\subsection{Pre-read}\label{pre-read}

\begin{itemize}
\tightlist
\item
  \textbf{Essential Question:} How can we reduce the complexity of data
  without losing its story?
\item
  \textbf{Vocabulary to Notice:} Eigenvector, Covariance, Orthogonality,
  Variance.
\item
  \textbf{Questions to Ponder:} Why is it harder to find patterns in 100
  dimensions than in 2? Can we ever truly reconstruct original data
  after it has been compressed?
\item
  \textbf{Linear Algebra:} Review dot products, eigenvectors, and
  eigenvalues.
\item
  \textbf{Statistics:} Refresh concepts of variance, covariance, and
  standard deviation.
\item
  \textbf{Tools:} Familiarize yourself with \texttt{scikit-learn}
  preprocessing modules.
\end{itemize}

\subsection{Learning Objectives}\label{learning-objectives}

By the end of this lecture, you will be able to: 1. \textbf{Explain} the
geometric intuition of PCA using the concepts of variance maximization
and projection error. 2. \textbf{Interpret} Principal Components as
linear combinations of original features rather than simple feature
selection. 3. \textbf{Apply} the ``Best Fit'' criteria to distinguish
between minimizing reconstruction error and maximizing explained
variance. 4. \textbf{Prepare} a high-dimensional dataset for PCA by
understanding the necessary preprocessing steps like centering.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{The Intuition of Dimensionality
Reduction}\label{the-intuition-of-dimensionality-reduction}

\subsubsection{Unsupervised Learning
Context}\label{unsupervised-learning-context}

In supervised learning, you have target labels (classification and
regression). However, PCA is a key part of \textbf{unsupervised
learning}, which typically involves three different techniques: 1.
\textbf{Clustering:} Grouping similar data points together (e.g.,
K-means). 2. \textbf{Dimensionality Reduction:} Reducing a high number
of features (e.g., PCA). 3. \textbf{Anomaly Detection:} Identifying
outliers in the data.

In unsupervised learning, you only have a set of inputs (\(X\)). While
clustering uses similarity to group data points, dimensionality
reduction takes multiple features (like \(X_1, X_2, X_3, X_4\)) and
condenses them into a smaller set of features, such as \(PC_1\) and
\(PC_2\).

\subsubsection{The ``Tea'' Analogy: Feature
Extraction}\label{the-tea-analogy-feature-extraction}

PCA is not about deleting features; it is about combining them. Think of
making a cup of tea (chai). You don't consume sugar, milk, water, and
tea powder independently. Instead, you mix them in a pan to create a
single concoction: \textbf{Tea}. * \textbf{Irreversible Combination:}
Once the tea is made, it is practically impossible to separate the sugar
back out or remove the milk. The new element is a mixture of all the
original ingredients. * \textbf{Retained Characteristics:} The tea has
the features of all four ingredients, yet it is a completely new, single
unit. * \textbf{The Eigenvector (The Recipe):} The slope of the
Principal Component line represents the ratio of original ingredients.
If the eigenvector is {[}0.8, 0.2{]}, it means the new dimension is 80\%
Movie 1 and 20\% Movie 2. This linear combination is the recipe for your
new feature.

\begin{quote}
ðŸ’¡ \textbf{Pro Tip:} Reducing dimensions is like choosing a condensed
book over a 100-page book. If you can get the same information in 10
pages, the machine learning model will train, learn, and infer much
faster.
\end{quote}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Variance Maximization and Geometric
Intuition}\label{variance-maximization-and-geometric-intuition}

\subsubsection{The Setup: Centering the
Data}\label{the-setup-centering-the-data}

Imagine a dataset with two features: \texttt{Movie\ 1\ Rating} and
\texttt{Movie\ 2\ Rating}. The first step in PCA is to calculate the
average for both features.

We then \textbf{shift the axis}. Instead of starting the origin at
\((0,0)\), we move the center to the mean of \(X_1\) and \(X_2\). *
\textbf{Relative Position:} Shifting the data does not change how the
points relate to each other; the highest and lowest points remain the
same. It simply centers the data to allow for rotation.

\subsubsection{Scaling to Unit Variance}\label{scaling-to-unit-variance}

Centering alone is often insufficient. If Feature A ranges from 0 to
1000 and Feature B ranges from 0 to 1, Feature A will dominate the
variance calculations simply due to its scale. * \textbf{Z-score
Normalization:} We divide each centered feature by its standard
deviation. This ensures all features contribute equally to the analysis,
preventing features with large magnitudes from skewing the Principal
Components.

\subsubsection{Finding the ``Best Fit''
Line}\label{finding-the-best-fit-line}

PCA draws a random line through this new origin and rotates it to find
the ``Best Fit.'' This is determined by two conditions: 1.
\textbf{Minimize Distance to the Line:} We want to minimize the distance
from the actual data point to the line (the projection distance). 2.
\textbf{Maximize Distance to the Origin:} We want to maximize the
distance from the origin to the point where the data is projected onto
the line.

\subsubsection{The Right-Angle
Connection}\label{the-right-angle-connection}

These two distances form a right-angle triangle where the distance from
the origin to the actual data point is the \textbf{hypotenuse}. Because
the hypotenuse is fixed, whenever we maximize the distance from the
projected point to the origin, we automatically minimize the distance
from the actual point to the line. This relationship is governed by the
Pythagorean theorem: since the distance from the origin to the data
point (hypotenuse) is constant, the squared projection distance and
squared error distance must sum to that constant.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{graph LR}
\NormalTok{    A[Standardize Data] {-}{-}\textgreater{} B[Compute Covariance Matrix]}
\NormalTok{    B {-}{-}\textgreater{} C[Calculate Eigenvectors/Values]}
\NormalTok{    C {-}{-}\textgreater{} D[Sort and Select Components]}
\NormalTok{    D {-}{-}\textgreater{} E[Project Data]}
\end{Highlighting}
\end{Shaded}

\subsection{Mathematical Transformation and
Proportions}\label{mathematical-transformation-and-proportions}

While we visualize this as rotating a line, the math involves finding
the best linear combination of your features.

\subsubsection{The Covariance Matrix and
Eigenvectors}\label{the-covariance-matrix-and-eigenvectors}

Mathematically, PCA finds the directions of maximum variance by
analyzing the \textbf{covariance matrix} of the data. 1.
\textbf{Covariance Matrix:} We compute a matrix representing how
variables vary together. 2. \textbf{Eigen-decomposition:} We calculate
the eigenvectors and eigenvalues of this matrix. * \textbf{Eigenvectors}
represent the directions (Principal Components). * \textbf{Eigenvalues}
represent the magnitude of variance in those directions.

The transformation can be expressed as: \(Z = XW\)

Where \(X\) is the standardized data matrix, \(W\) is the matrix of
eigenvectors (weights), and \(Z\) contains the new Principal Component
scores.

\subsubsection{Linear Combinations}\label{linear-combinations}

The Principal Component (\(PC_1\)) is a linear combination of your
original features defined by the eigenvector weights. For example, if
the weight vector is \([0.97, 0.24]\), \(PC_1\) is dominated by \(X_1\)
but influenced by \(X_2\). This allows you to represent the data using
the projected points on the PC line instead of the original data
coordinates.

\subsubsection{Orthogonality and
Components}\label{orthogonality-and-components}

\begin{itemize}
\tightlist
\item
  \textbf{PC1:} The best-fit line that meets the
  maximization/minimization criteria.
\item
  \textbf{PC2:} A line drawn at a 90-degree angle (perpendicular) to
  PC1.
\end{itemize}

\textbf{Dimensionality Reduction Rule:} If you provide \(N\) features,
PCA will generate \(N\) Principal Components. However, you do not need
all of them. You can take 10 dimensions and reduce them to 3, or even 1,
by discarding the components that do not add value (like PC2 in a
2D-to-1D reduction).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Information Loss and Component
Selection}\label{information-loss-and-component-selection}

A common concern in PCA is whether information is lost during the
reduction process.

\subsubsection{The Dilution Analogy}\label{the-dilution-analogy}

Think of adding water to milk. There is a slight change in taste---a
``dilution'' or loss of the original milk's intensity. However, as long
as the loss is negligible and we can still taste the milk, we accept it
for the sake of the new mixture. * In PCA, transforming data from 10
dimensions to 1 dimension involves some loss of variation, but this is
usually negligible compared to the computational gains.

\subsubsection{Selecting Components: The Scree
Plot}\label{selecting-components-the-scree-plot}

We determine how many components to keep by analyzing the
\textbf{Explained Variance Ratio}. * \textbf{Scree Plot:} A graph
plotting the eigenvalues (variance explained) against the component
number. * \textbf{Elbow Method:} Look for the ``elbow'' in the
plot---the point where adding more components yields diminishing returns
in explained variance. * \textbf{Cumulative Variance:} Often, we select
enough components to explain a threshold of variance (e.g., 95\%).

\begin{quote}
\textbf{Visualization:} Imagine a bar chart where the first bar
(\(PC_1\)) is very tall, the second (\(PC_2\)) is shorter, and
subsequent bars flatten out. You cut off the selection where the curve
flattens.
\end{quote}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Implementation Example}\label{implementation-example}

This example implements PCA using Python's \texttt{scikit-learn}
library, including the critical scaling step.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{from}\NormalTok{ sklearn.decomposition }\ImportTok{import}\NormalTok{ PCA}
\ImportTok{from}\NormalTok{ sklearn.preprocessing }\ImportTok{import}\NormalTok{ StandardScaler}
\ImportTok{from}\NormalTok{ sklearn.datasets }\ImportTok{import}\NormalTok{ load\_breast\_cancer}

\CommentTok{\# 1. Load Data}
\NormalTok{data }\OperatorTok{=}\NormalTok{ load\_breast\_cancer()}
\NormalTok{X }\OperatorTok{=}\NormalTok{ data.data}

\CommentTok{\# 2. Preprocessing: Standardization (Crucial for PCA)}
\NormalTok{scaler }\OperatorTok{=}\NormalTok{ StandardScaler()}
\NormalTok{X\_scaled }\OperatorTok{=}\NormalTok{ scaler.fit\_transform(X)}

\CommentTok{\# 3. Apply PCA}
\CommentTok{\# Let\textquotesingle{}s reduce 30 dimensions to 2}
\NormalTok{pca }\OperatorTok{=}\NormalTok{ PCA(n\_components}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{X\_pca }\OperatorTok{=}\NormalTok{ pca.fit\_transform(X\_scaled)}

\CommentTok{\# 4. Analyze Results}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Original Shape: }\SpecialCharTok{\{}\NormalTok{X}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)       }\CommentTok{\# (569, 30)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Reduced Shape:  }\SpecialCharTok{\{}\NormalTok{X\_pca}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)   }\CommentTok{\# (569, 2)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Explained Variance Ratio: }\SpecialCharTok{\{}\NormalTok{pca}\SpecialCharTok{.}\NormalTok{explained\_variance\_ratio\_}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\CommentTok{\# Output might look like: [0.4427 0.1897] {-}\textgreater{} PC1 explains 44\%, PC2 explains 19\%}
\end{Highlighting}
\end{Shaded}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Synthesis Points}\label{synthesis-points}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Solution to the Curse of Dimensionality:} PCA combats the
  sparsity and computational complexity of high-dimensional data by
  projecting it onto a lower-dimensional subspace while preserving
  maximum variance.
\item
  \textbf{Trade-off Between Interpretability and Performance:} While PCA
  improves model training speed and generalization by removing noise,
  the resulting Principal Components are abstract linear combinations
  (\(Z = XW\)), making them harder to interpret than original features.
\item
  \textbf{Geometric Optimization:} The algorithm simultaneously
  minimizes reconstruction error (distance to line) and maximizes
  explained variance (spread of projections), mathematically derived via
  the eigenvectors of the covariance matrix.
\item
  \textbf{Preprocessing Dependency:} PCA is highly sensitive to scale.
  Standardization (Z-score normalization) is mandatory to prevent
  features with large magnitudes from artificially dominating the
  variance analysis.
\item
  \textbf{Strategic Information Retention:} Through tools like Scree
  Plots and Cumulative Variance thresholds, practitioners can make
  informed decisions about how much information to sacrifice for
  efficiency.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Practice Lab: Dimensionality Reduction
Challenge}\label{practice-lab-dimensionality-reduction-challenge}

\textbf{Scenario:} You are working with the MNIST dataset (handwritten
digits), which has 784 features per image (28x28 pixels). Your goal is
to visualize this high-dimensional data in 2D and speed up a classifier.

\textbf{Assignments:} 1. \textbf{Preprocessing:} Load the MNIST dataset
(use \texttt{sklearn.datasets.fetch\_openml}) and apply
\texttt{StandardScaler}. 2. \textbf{Variance Analysis:} Fit a PCA model
without specifying \texttt{n\_components}. Plot the cumulative explained
variance ratio. Determine how many components are needed to explain 95\%
of the variance. 3. \textbf{Visualization:} Reduce the data to 2
dimensions (\texttt{n\_components=2}) and plot a scatter plot, coloring
points by their digit label. Does the 2D projection show clear
separation between digits? 4. \textbf{Impact Assessment:} Train a simple
Logistic Regression model on the original data vs.~the PCA-reduced data
(95\% variance). Compare training time and accuracy.

\textbf{Constraints:} * Complete the challenge in 60 minutes. * Use
\texttt{matplotlib} or \texttt{seaborn} for visualizations.

\end{document}
